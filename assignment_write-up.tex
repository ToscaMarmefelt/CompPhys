%%% Preamble
\documentclass[paper=a4, fontsize=11pt]{scrartcl}
\usepackage[T1]{fontenc}
\usepackage{fourier}

\usepackage[english]{babel}															% English language/hyphenation
\usepackage[protrusion=true,expansion=true]{microtype}	
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage[pdftex]{graphicx}	
\usepackage{url}


%%% Custom sectioning
\usepackage{sectsty}
\allsectionsfont{\centering \normalfont\scshape}


%%% Custom headers/footers (fancyhdr package)
\usepackage{fancyhdr}
\pagestyle{fancyplain}
\fancyhead{}											% No page header
\fancyfoot[L]{}											% Empty 
\fancyfoot[C]{}											% Empty
\fancyfoot[R]{\thepage}									% Pagenumbering
\renewcommand{\headrulewidth}{0pt}			% Remove header underlines
\renewcommand{\footrulewidth}{0pt}				% Remove footer underlines
\setlength{\headheight}{13.6pt}

%Added by me
\usepackage{bbold}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}


%%% Equation and float numbering
%\numberwithin{equation}{section}		% Equationnumbering: section.eq#
%\numberwithin{figure}{section}			% Figurenumbering: section.fig#
%\numberwithin{table}{section}				% Tablenumbering: section.tab#


%%% Maketitle metadata
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} 	% Horizontal rule

\title{
		%\vspace{-1in} 	
		\usefont{OT1}{bch}{b}{n}
		\normalfont \normalsize \textsc{Department of Physics $|$ Imperial College London} \\ [25pt]
		\horrule{0.5pt} \\[0.4cm]
		\huge Computational Physics: Assignment \\
		\horrule{2pt} \\[0.5cm]
}
\author{
		\normalfont 								\normalsize
        Tosca Cederberg Marmefelt\\[-3pt]		\normalsize
        CID: $01204044$\\[-3pt]		\normalsize
        \today
}
\date{}


%%% Begin document
\begin{document}
\maketitle
\section{Floating point variables}
The floating-point machine accuracy of a system is defined as the smallest value that can be meaningfully added to or subtracted from $1$, and it varies depending on the floating-point precision used. In the table below, the composition of the four most common precisions are specified. 

\begin{center}
	\label{fact2}
	\begin{tabular}{l || c | c | c | c | c }
		\textbf{Precision} & \textbf{Sign} & \textbf{Mantissa} & \textbf{Exponent} & \textbf{Total} & \textbf{Numpy Data Type} \\ \hline \hline
		Half & $1$ bit & $10$ bits & $5$ bits & $16$ bits & \url{float16} \\ \hline 
		Single & $1$ bit & $23$ bits & $8$ bits & $32$ bits & \url{float32} \\ \hline 
		Double & $1$ bit & $52$ bits & $11$ bits & $64$ bits & \url{float64} \\ \hline 
		Extended & $1$ bit & $112$ bits & $15$ bits & $128$ bits & Not available in Python
	\end{tabular}
\end{center}
The machine accuracy for the three different precisions which are defined in Python was calculated by adding decreasing powers of $2$ to $1$ until the sum no longer differed from $1$. the results are presented in the table below. 
\begin{center}
	\label{result2}
	\begin{tabular}{l || c | c}
	\textbf{Precision} & \textbf{Machine Accuracy}  & \textbf{Smallest Value Meaningfully Subtracted from $1$}\\ \hline \hline
	Half & $0.0009765625$ & $0.00048828125$ \\ \hline
	Single & $1.1920928955078125 \times 10^{-7}$ & $ 5.960464477539063 \times 10^{-8}$\\ \hline
	Dounble & $2.220446049250313 \times 10^{-16}$ & $1.1102230246251565 \times 10^{-16}$
	\end{tabular}
\end{center}
As indicated in the result table above, the smallest number meaningfully added to $1$ (i.e. $1.000...01$) differs from the smallest value meaningfully subtracted from $1$ (i.e. $0.999...99$) by a factor of $2$. This is simply due to the fact that the most significant bit in the mantissa corresponding to $1.000...01$ is $2^{0}$, compared to $2^{-1}$ for $0.999...99$. Hence it follows that the smallest value meaningfully subtracted from $1$ is smaller than thad added to $1$ by a factor of $2$, as observed. Furthermore, the machine accuracy quoted above agrees with the expected values of $2^{-\text{(mantissa)}}$ for each precision. 



\section{Matrix methods}
Consider the matrix
\begin{align}
\label{A define}
	A = 
	\begin{bmatrix}
	3 & 1 & 0 & 0 & 0 \\
  	3 & 9 & 4 & 0 & 0 \\
  	0 & 9 & 20 & 10 & 0 \\
  	0 & 0 & -22 & 31 & -25 \\
  	0 & 0 & 0 & -55 & 60
	\end{bmatrix}  \text{ .}
\end{align}
Following the reasoning provided in the lecture notes, we realise that the square matrix given above may be decomposed and written in terms of a product of a lower and an upper matrix, $L$ and $U$ respectively. Using Crout's method for decomposition, i.e. letting the diagonal entries of the lower matrix all be equal to $1$, the resulting matrix is given by
\begin{align}	
	A \equiv LU =
		\begin{bmatrix}
	3 & 1 & 0 & 0 & 0 \\
	1 & 8 & 4 & 0 & 0 \\
	0 & 1.13 & 15.5 & 10 & 0 \\
	0 & 0 & -1.42 & 45.2 & -25 \\
	0 & 0 & 0 & -1.22 & 29.6
	\end{bmatrix}  \text{ .}
\end{align}
Given the decomposed form of the matrix, the determinant can easily be computated as simply being the product of all diagonal entries ($\beta_{jj}$) in the upper matrix, as given in Equation \ref{det}. 
\begin{equation}
\label{det}
\text{det}(A) = \prod_{j=0}^{N-1} \beta_{jj} = 497220.0
\end{equation}

Decomposing a matrix into an upper and a lower matrix is a useful method for simplifying solving matrix equations $A \textbf{x} = \textbf{b}$ for a large number of \textbf{b}. By writing a routine using backward and forward substitution, the matrix equation presented in the assignment was solved as indicated in Equation \ref{matrix equation}. 

\begin{align}
\label{matrix equation}
A \textbf{x} \equiv LU \textbf{x} = 
	\begin{bmatrix}
	2 \\
	5 \\
	-4 \\
	8 \\
	9
	\end{bmatrix}
	\implies \textbf{x} =
	\begin{bmatrix}
	-0.268 \\
	1.10 \\
	-0.495 \\
	0.0410 \\
	0.304
	\end{bmatrix}
\end{align}

In order to calculate the inverse of our matrix $A$, we recall the fact that
\begin{equation}
A^{-1} A = \mathbb{1} \text{ ,}
\end{equation}
where $\mathbb{1}$ is the $N \times N$ identity matrix. For the matrix defined in Equation \ref{A define}, this allows us to set up $N$ equations of the type
\begin{equation}
A^{-1}\text{[\textit{n}] } LU = \mathbb{1}\text{[\textit{n}]} \text{ ,}
\end{equation}
for each of the $n = 0, 1, ... , (N-1)$ columns in $A^{-1}$. Recognising that this is the same type of equation as that set up in Equation \ref{matrix equation}, we can use the same routine to loop through each column in $A^{-1}$ to find 

\begin{align}
\label{A inverse}
A^{-1} = 
\begin{bmatrix}
0.33 & -0.13 & 0.043 & -0.0037 & -0.012 \\
0 & 0.13 & -0.065 & 0.011 & 0.014 \\
0 & 0 & 0.065 & -0.022 & -0.0067 \\
0 & 0 & 0 & 0 & 0.022 \\
0 & 0 & 0 & 0 & 0.034
\end{bmatrix}
\end{align}

\section{Interpolation}
\subsection{Linear interpolation}
Recalling that a linear function may be written as 
\begin{equation}
f(x) = kx + m \text{ ,}
\end{equation}
for constants $k$ and $m$, we may define a linear function $f_{i}(x)$ between each pair of points $i$ and $(i+1)$ for $i = 0, 1, ... , (N-2)$, where $N$ is the number of data points. Each linear function is specified by $k_{i}$ and $m_{i}$. When plotting the interpolated function for a set of $x$-values, which function $f_{i}(x)$ to call is determined depending on which interval $[x_{i}, x_{i+1}]$ the value $x$ lies in, as is illustrated in Figure \ref{interpolation theory}. 
\begin{figure}
	\centering
	\includegraphics[width=10cm]{interpolation_theory.pdf}
	\caption{\footnotesize{Simple example of $N=5$ data points $(x_{i}, y_{i})$ for $i = 0, 1, ... , (N-1)$. An interpolating function $f_{i}$ has been defined between each pair of points $i$ and $(i+1)$. The red arrows indicate which function $f_{i}$ to call when plotting the interpolated function over a greater number of $x$-values in the specified range.}}
	\label{interpolation theory}
\end{figure}

\subsection{Cubic spline interpolation}

\begin{align}
\label{cubic}
	\begin{split}
	\frac{x_{i}-x_{i-1}}{6} f_{i-1}" + \frac{x_{i+1}-x_{i-1}}{3} f_{i}" + 	\frac{x_{i+1}-x_{i}}{6} f_{i+1}"  &= \frac{f_{i+1}-f_{i}}{ x_{i+1}-x_{i} } - \frac{f_{i}-f_{i-1}}{ x_{i}-x_{i-1} } \\
	a_{i} f_{i-1}" + b_{i} f_{i}" + c_{i} f_{i+1}" &= F_{i}
	\end{split}
\end{align}

The code was tested using a data set of $6$ data points to facilitate test calculations by hand. Equation \ref{natural  spline bc} below shows how we can reduce the matrix equation generated as a result of setting up Equation \ref{cubic} for each of the $N$ data points provided into an $(N-1) \times (N-1)$ matrix equation using the natrual spline boundary condition, i.e. letting $f_{0}" = f_{N=6}" = 0$. 
\begin{align}	
\label{natural spline bc}
A \textbf{\textit{f}"} =
\begin{bmatrix}
a_{1} & b_{1} & c_{1} & 0 & 0 & 0 & 0 \\
0 & a_{2} & b_{2} & c_{2} & 0 & 0 & 0\\
0 & 0 & a_{3} & b_{3} & c_{3} &0 & 0\\
0 & 0 & 0 & a_{4} & b_{4} & c_{4} & 0 \\
0 & 0 & 0 & 0 & a_{5} & b_{5} & c_{5}
\end{bmatrix}
%\cdot
\begin{bmatrix}
f_{0}" \\
f_{1}" \\
f_{2}" \\
f_{3}" \\
f_{4}" \\
f_{5}" \\
f_{6}"
\end{bmatrix}
\longrightarrow
\begin{bmatrix}
 b_{1} & c_{1} & 0 & 0 & 0 \\
 a_{2} & b_{2} & c_{2} & 0 & 0\\
 0 & a_{3} & b_{3} & c_{3} &0 \\
 0 & 0 & a_{4} & b_{4} & c_{4} \\
 0 & 0 & 0 & a_{5} & b_{5}
\end{bmatrix}
%\cdot
\begin{bmatrix}
f_{1}" \\
f_{2}" \\
f_{3}" \\
f_{4}" \\
f_{5}" 
\end{bmatrix}
\end{align}
Recognising that using the natural spline boundary condition gives us a square matrix equation, it is clear that we can use the matrix solver routine defined in Question 2 to solve the equation $A\textbf{\textit{f}"} = \textbf{\textit{F}}$ for $\textbf{\textit{f}"}$. 

\section{Fourier transforms}
Given the signal function $h(t)$ and response function $g(t)$, we recognise that the convolution of the two is easiest calculated using the Convolution theorem:
\begin{equation}
\big(g*h\big)(t) = \mathcal{F}^{-1} \Big( \mathcal{F} (g) \cdot \mathcal{F} (h) \Big) \text{ ,}
\end{equation}
where $\mathcal{F}$ and $\mathcal{F}^{-1}$ indicate the Fourier transform and inverse Fourier transform respectively. Using the external library \url{numpy.fft}, the two functions $h(t)$ and $g(t)$ as specified in Figure \ref{fourier graph} (a) and (b) were convoluted to produce the convoluted graph in Figure \ref{fourier graph} (c). 

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{fourier_graph.pdf}
	\caption{\footnotesize{(a) Signal function h(t) in the time domain. (b) Response function g(t) in the time domain. (c) The convolution of h(t) and g(t) evaluated via a simple routine using the Convolution Theorem. When plotted on top of the original signal function, it is clear that the signal has become smeared as specified by the response function, as would be expected.}}
	\label{fourier graph}
\end{figure}

Since the Fast Fourier Transform used in \url{numpy.fft} requires a sample size which is a factor of $2$, the respective functions were discretely sampled at $N=2^{6}$ evenly spaced values of $t$ in the range $t \in [-8,8]$. Hence there was no need for padding. 

\section{Random numbers}
\subsection{Uniform distribution}
Using the uniformly distributed random number generator \url{random.random()} which provides a random number in the range $[0,1]$, a sample of $10^{5}$ deviates were generated. These are plotted in Figure \ref{random graph} (a), where the bin ranges were determined using the Freedman-Diaconis rule (minimising the difference in area under empirical and theoretical pdf) via the external  \url{astrophys.stats} library. 

\subsection{Transformation method}
Given the probability distribution function
\begin{equation}
\label{pdf trans}
pdf(x) = \frac{1}{2} \sin(x) \text{ ,}
\end{equation}
and a uniformly distributed $x \in [0,\pi]$, the fundamental transformation law of probabilities allows us to define 
\begin{equation}
| P(y) dy | = |P(x) dx | \implies |\frac{1}{2} \sin{y} | = |U(x) dx| \text{ .}
\end{equation}
Assuming $\frac{dy}{dx} \geq 0$, then we may integrate and invert the result to find
\begin{equation}
y(x) = \arccos{ 1-2x }
\end{equation}
where $y(x)$ is a new deviate distributed according to the PDF defined in Equation \ref{pdf trans}. This is plotted for a sample of $10^{5}$ points in Figure \ref{random graph} (b). Again, the bin ranges were decided based on the Freedman-Diaconis rule. 

\subsection{Rejection method}
Now consider the PDF
\begin{equation}
\label{pdf rejec}
pdf(x) = \frac{2}{\pi} \sin^{2}{x}
\end{equation}
again defined for a uniformly distributed $x \in [0,\pi]$. The comparison function was set to be $c(y) = \frac{2}{\pi} \sin{y}$ to ensure $c(y) > P(y)$ for all $y$ (where $P(y)$ is the PDF defined in Equation \ref{pdf rejec}). Normalising the comparison function to give $c_{norm}(y) = \frac{1}{2} \sin{y}$, we may pick the random deviate $y_{i}$ as
\begin{equation}
y_{i} = \arccos{\big( 1-2 \cdot \text{ \url{random.random()} } \big)} \text{ .}
\end{equation}
We then pick a random value $p_{i}$ in the range $[0, c(y_{i})]$ via
\begin{equation}
p_{i} = \frac{2}{\pi} \sin{(y_{i})} \cdot \text{\url{random-random()}} \text{ .}
\end{equation}
The deviates $y_{i}$ which satisfy $P(y_{i}) \geq p_{i}$ are then accepted by the rejection method. In Figure \ref{random graph}, $N=10^{5}$ such points are plotted. The efficiency of the rejection method is proportional to the time is takes to generate the $N$ accepted deviates. Comparing to the number of values which had to be generated to create the plot in Figure \ref{random graph} (b) (i.e. the same as the total number of accepted points in our rejection method), the efficiency of the rejection method was given by:
\begin{equation}
efficiency= 0.7870483326381073
\end{equation}
which is the same as the fraction of the area under the PDF in (c) compared to the area under the PDF in (b), as was expected. 

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{random_graph.pdf}
	\caption{\footnotesize{hello}}
	\label{random graph}
\end{figure}

%%% End document
\end{document}